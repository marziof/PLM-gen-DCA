{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with separate heads for domain1, domain2, interdomain\n"
     ]
    },
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\nUntyped global name 'tokenize_seqs': Cannot determine Numba type of <class 'function'>\n\nFile \"../../../../../../../tmp/ipykernel_15632/1050797239.py\", line 219:\n<source missing, REPL/exec in use?> \n\nThis error may have been caused by the following argument(s):\n- argument 0: Cannot determine Numba type of <class '__main__.MultiDomainAttentionSubBlock'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 313\u001b[0m\n\u001b[1;32m    309\u001b[0m MSA_A, MSA_B \u001b[38;5;241m=\u001b[39m paired_fasta_to_labeled_array(test_fasta, L_A\u001b[38;5;241m=\u001b[39mL_A)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# MSA_A => shape (N, L_A), MSA_B => shape(N, L_B)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# (C) get correct vs. incorrect energies\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m corr_scores, incorr_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtest_interdomain_energy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMSA_A\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMSA_B\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorrect_criterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# (D) Plot hist\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/master_project2/lib/python3.12/site-packages/numba/core/dispatcher.py:423\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    419\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis error may have been caused \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby the following argument(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00margs_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    421\u001b[0m         e\u001b[38;5;241m.\u001b[39mpatch_message(msg)\n\u001b[0;32m--> 423\u001b[0m     \u001b[43merror_rewrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtyping\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# Something unsupported is present in the user code, add help info\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     error_rewrite(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsupported_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/master_project2/lib/python3.12/site-packages/numba/core/dispatcher.py:364\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args.<locals>.error_rewrite\u001b[0;34m(e, issue_type)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\nUntyped global name 'tokenize_seqs': Cannot determine Numba type of <class 'function'>\n\nFile \"../../../../../../../tmp/ipykernel_15632/1050797239.py\", line 219:\n<source missing, REPL/exec in use?> \n\nThis error may have been caused by the following argument(s):\n- argument 0: Cannot determine Numba type of <class '__main__.MultiDomainAttentionSubBlock'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numba import prange, jit\n",
    "\n",
    "#######################################################\n",
    "# 1) Helper function: read_tensor_from_txt\n",
    "#######################################################\n",
    "def read_tensor_from_txt(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Read the dimensions from the first line\n",
    "    dims = list(map(int, lines[0].strip().split()))\n",
    "    \n",
    "    tensor_data = []\n",
    "    current_slice = []\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Slice\"):\n",
    "            if current_slice:\n",
    "                tensor_data.append(current_slice)\n",
    "                current_slice = []\n",
    "        elif line:\n",
    "            current_slice.append(list(map(float, line.split(','))))\n",
    "    if current_slice:\n",
    "        tensor_data.append(current_slice)\n",
    "\n",
    "    tensor = torch.tensor(tensor_data).view(*dims)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 2) The multi-domain sub-block model\n",
    "#######################################################\n",
    "class MultiDomainAttentionSubBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single-layer attention model that splits H heads among 3 groups:\n",
    "      - Domain1 heads => Q1,K1,V1\n",
    "      - Domain2 heads => Q2,K2,V2\n",
    "      - Inter-domain heads => Qint1,Kint1,Vint1 (domain1->domain2),\n",
    "                              Qint2,Kint2,Vint2 (domain2->domain1)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        H=32,          # total heads\n",
    "        d=23,          # dimension for Q,K\n",
    "        N=176,         # total protein length\n",
    "        q=22,          # amino-acid alphabet\n",
    "        lambd=0.001,\n",
    "        domain1_end=63, \n",
    "        H1=10, \n",
    "        H2=20, \n",
    "        device='cpu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.d = d\n",
    "        self.N = N\n",
    "        self.q = q\n",
    "        self.lambd = lambd\n",
    "        self.device = device\n",
    "\n",
    "        # domain1 is [0..domain1_end], length => N_alpha\n",
    "        self.domain1_end = domain1_end\n",
    "        self.domain2_start = domain1_end + 1\n",
    "        self.N_alpha = self.domain2_start\n",
    "        self.N_beta = self.N - self.N_alpha\n",
    "\n",
    "        self.H1 = H1\n",
    "        self.H2 = H2\n",
    "\n",
    "        # (1) Domain1 heads\n",
    "        self.Q1 = nn.Parameter(torch.randn(H1, d, self.N_alpha, device=self.device))\n",
    "        self.K1 = nn.Parameter(torch.randn(H1, d, self.N_alpha, device=self.device))\n",
    "        self.V1 = nn.Parameter(torch.randn(H1, q, q, device=self.device))\n",
    "\n",
    "        # (2) Domain2 heads\n",
    "        num_dom2_heads = self.H2 - self.H1\n",
    "        self.Q2 = nn.Parameter(torch.randn(num_dom2_heads, d, self.N_beta, device=self.device))\n",
    "        self.K2 = nn.Parameter(torch.randn(num_dom2_heads, d, self.N_beta, device=self.device))\n",
    "        self.V2 = nn.Parameter(torch.randn(num_dom2_heads, q, q, device=self.device))\n",
    "\n",
    "        # (3) Inter-domain heads\n",
    "        num_inter_heads = self.H - self.H2\n",
    "        num_inter_heads1 = num_inter_heads // 2  # domain1->domain2\n",
    "        num_inter_heads2 = num_inter_heads - num_inter_heads1  # domain2->domain1\n",
    "\n",
    "        self.Qint1 = nn.Parameter(torch.randn(num_inter_heads1, d, self.N_alpha, device=self.device))\n",
    "        self.Kint1 = nn.Parameter(torch.randn(num_inter_heads1, d, self.N_beta, device=self.device))\n",
    "        self.Vint1 = nn.Parameter(torch.randn(num_inter_heads1, q, q, device=self.device))\n",
    "\n",
    "        self.Qint2 = nn.Parameter(torch.randn(num_inter_heads2, d, self.N_beta, device=self.device))\n",
    "        self.Kint2 = nn.Parameter(torch.randn(num_inter_heads2, d, self.N_alpha, device=self.device))\n",
    "        self.Vint2 = nn.Parameter(torch.randn(num_inter_heads2, q, q, device=self.device))\n",
    "\n",
    "    def forward(self, Z, weights, head_group='all'):\n",
    "        \"\"\"\n",
    "        Normally you do partial updates. We'll skip the forward logic here \n",
    "        since we only want the sub-block parameters for the energy test.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 3) A function to compute \"domain1->domain2\" \n",
    "#    inter-domain energy for a single seq\n",
    "#######################################################\n",
    "def compute_energy_inter_subblock_domain12(seqAB, model):\n",
    "    \"\"\"\n",
    "    seqAB: shape (model.N_alpha + model.N_beta,) of amino-acid indices\n",
    "           The first model.N_alpha = domain1, next model.N_beta = domain2\n",
    "\n",
    "    Returns a scalar float of the \"inter-domain\" energy for domain1->domain2\n",
    "    based on Qint1,Kint1,Vint1, using a vectorized approach.\n",
    "    \"\"\"\n",
    "    # 1) Split the sequence\n",
    "    seqA = seqAB[: model.N_alpha]  # domain1\n",
    "    seqB = seqAB[model.N_alpha:]   # domain2\n",
    "\n",
    "    # 2) Build raw logits => e_sel: shape (L1, L2, #heads)\n",
    "    Q_sel = model.Qint1    # (#heads_int1, d, L1)\n",
    "    K_sel = model.Kint1    # (#heads_int1, d, L2)\n",
    "    V_sel = model.Vint1    # (#heads_int1, q, q)\n",
    "    e_sel = torch.einsum('hdi,hdj->ijh', Q_sel, K_sel)  # (L1,L2,#heads_int1)\n",
    "\n",
    "    # 3) Softmax over j => shape (L1,L2,#heads_int1)\n",
    "    sf = torch.softmax(e_sel, dim=1)\n",
    "\n",
    "    # 4) Vectorized gather of V_sel[h, a_i, a_j]\n",
    "    #    - We'll permute sf to (h, L1, L2)\n",
    "    sf_t = sf.permute(2, 0, 1)  # => (num_heads, L1, L2)\n",
    "\n",
    "    # (a) Reshape seqA and seqB to enable broadcast\n",
    "    #     seqA_2D => shape (L1,1), seqB_2D => shape(1,L2)\n",
    "    seqA_2D = seqA.view(-1,1)  # shape (L1,1)\n",
    "    seqB_2D = seqB.view(1,-1)  # shape (1,L2)\n",
    "\n",
    "    # (b) For each head h, we want V_sel[h][seqA_2D, seqB_2D]\n",
    "    #     That yields shape (L1,L2). We'll stack these into (num_heads, L1, L2).\n",
    "    #     We can do it with a list comprehension and torch.stack:\n",
    "    all_vals = []\n",
    "    num_heads = V_sel.shape[0]\n",
    "    for h in range(num_heads):\n",
    "        # gather => shape (L1, L2)\n",
    "        # indexing with (seqA_2D, seqB_2D) picks V_sel[h, a_i, a_j] for each i,j\n",
    "        vals_h = V_sel[h][seqA_2D, seqB_2D]  \n",
    "        all_vals.append(vals_h)\n",
    "\n",
    "    # shape => (num_heads, L1, L2)\n",
    "    VAL = torch.stack(all_vals, dim=0)\n",
    "\n",
    "    # 5) Multiply (num_heads, L1, L2) by sf_t => (num_heads, L1, L2) and sum\n",
    "    energy_tensor = sf_t * VAL\n",
    "    # Single .sum() to get final scalar\n",
    "    energy_val = energy_tensor.sum().item()\n",
    "\n",
    "    return energy_val\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 4) Example usage: \n",
    "#    Compare \"correct vs. random pairs\" in a paired MSA\n",
    "#######################################################\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "tokens_protein = \"ACDEFGHIKLMNPQRSTVWY-\"\n",
    "\n",
    "def encode_sequence_lore(sequence : str, tokens : str=tokens_protein):\n",
    "    letter_map = {l : n for n, l in enumerate(tokens)}\n",
    "    return np.array([letter_map[l] for l in sequence])\n",
    "\n",
    "def tokenize_seqs(seqs, tokens=tokens_protein):\n",
    "    # seqs: shape (N, L)\n",
    "    seqs=seqs.astype(str)\n",
    "    seqs_str = np.array([\"\".join(row) for row in seqs])\n",
    "    seqs_tks = []\n",
    "    for s in seqs_str:\n",
    "        arr = encode_sequence_lore(s, tokens=tokens)\n",
    "        seqs_tks.append(arr)\n",
    "    return np.array(seqs_tks)\n",
    "\n",
    "def paired_fasta_to_labeled_array(fasta_name, L_A):\n",
    "    \"\"\"\n",
    "    Loads a FASTA of domainA||domainB concatenated. \n",
    "    Returns two arrays: MSA_A_label, MSA_B_label with shape (N, L_A) or (N, L_B).\n",
    "    Each row has 2 'label' columns in front if needed, or you can skip them.\n",
    "    For brevity, let's keep it simple.\n",
    "    \"\"\"\n",
    "    seqs=[]\n",
    "    IDs=[]\n",
    "    for record in SeqIO.parse(fasta_name, \"fasta\"):\n",
    "        seqs.append(record.seq)\n",
    "        IDs.append(record.description)\n",
    "    seqs=np.array(seqs)\n",
    "\n",
    "    MSA_A = seqs[:,:L_A]  # domain1\n",
    "    MSA_B = seqs[:,L_A:]  # domain2\n",
    "    return MSA_A, MSA_B\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def test_interdomain_energy(model, MSA_A, MSA_B, correct_criterion=\"index\"):\n",
    "    \"\"\"\n",
    "    MSA_A: shape (N, L1)\n",
    "    MSA_B: shape (N, L2)\n",
    "    If correct_criterion=\"index\", we assume row i in A is correct partner of row i in B.\n",
    "    We test all pairs => (i,j). \n",
    "    \"\"\"\n",
    "    L1 = model.N_alpha\n",
    "    L2 = model.N_beta\n",
    "\n",
    "    # tokenize\n",
    "    A_tks = tokenize_seqs(MSA_A)  # => shape(N,L1)\n",
    "    B_tks = tokenize_seqs(MSA_B)  # => shape(N,L2)\n",
    "\n",
    "    scores = []\n",
    "    correct_flags = []\n",
    "    N_A = A_tks.shape[0]\n",
    "    N_B = B_tks.shape[0]\n",
    "\n",
    "    for i in range(N_A):\n",
    "        seqA = A_tks[i]\n",
    "        for j in range(N_B):\n",
    "            seqB = B_tks[j]\n",
    "            # concatenate\n",
    "            seqAB = torch.tensor(np.concatenate([seqA, seqB]), dtype=torch.long)\n",
    "            # compute domain1->domain2 energy\n",
    "            val = compute_energy_inter_subblock_domain12(seqAB, model)\n",
    "\n",
    "            scores.append(val)\n",
    "            if correct_criterion==\"index\":\n",
    "                # correct if i==j\n",
    "                is_correct = (i==j)\n",
    "            else:\n",
    "                # or do a species label check, etc.\n",
    "                is_correct = False\n",
    "            correct_flags.append(is_correct)\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    correct_flags = np.array(correct_flags)\n",
    "\n",
    "    corr_scores = scores[correct_flags]\n",
    "    incorr_scores = scores[~correct_flags]\n",
    "\n",
    "    return corr_scores, incorr_scores\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 5) Putting it all together in \"main\"\n",
    "#######################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "\n",
    "    # (A) Suppose we read Qint1,Kint1,Vint1,... from disk\n",
    "    cwd = os.getcwd()\n",
    "    H=60\n",
    "    d=12\n",
    "    N=174\n",
    "    q=22\n",
    "    domain1_end=63\n",
    "    H1=25\n",
    "    H2=H1+15\n",
    "    family = \"HKRR_25_15_20_but_with_new_NEW_model_sep_optimizer_newreg_withHKRRtrainingfasta_d12_500batch\"\n",
    "    n_epochs=500\n",
    "    loss_type=\"without_J\"\n",
    "\n",
    "    # read them:\n",
    "    Qint1 = read_tensor_from_txt( f\"{cwd}/results/{H}_{d}_{family}_{loss_type}_{n_epochs}/Qint1_tensor.txt\" )\n",
    "    Kint1 = read_tensor_from_txt( f\"{cwd}/results/{H}_{d}_{family}_{loss_type}_{n_epochs}/Kint1_tensor.txt\" )\n",
    "    Vint1 = read_tensor_from_txt( f\"{cwd}/results/{H}_{d}_{family}_{loss_type}_{n_epochs}/Vint1_tensor.txt\" )\n",
    "\n",
    "    Qint2 = read_tensor_from_txt( f\"{cwd}/results/{H}_{d}_{family}_{loss_type}_{n_epochs}/Qint2_tensor.txt\" )\n",
    "    Kint2 = read_tensor_from_txt( f\"{cwd}/results/{H}_{d}_{family}_{loss_type}_{n_epochs}/Kint2_tensor.txt\" )\n",
    "    Vint2 = read_tensor_from_txt( f\"{cwd}/results/{H}_{d}_{family}_{loss_type}_{n_epochs}/Vint2_tensor.txt\" )\n",
    "\n",
    "    device='cpu'\n",
    "    model = MultiDomainAttentionSubBlock(\n",
    "        H=H, d=d, N=N, q=q,\n",
    "        domain1_end=domain1_end,\n",
    "        H1=H1, H2=H2,\n",
    "        device=device\n",
    "    ).to(device)\n",
    "\n",
    "    # assign them:\n",
    "    model.Qint1.data = Qint1\n",
    "    model.Kint1.data = Kint1\n",
    "    model.Vint1.data = Vint1\n",
    "    model.Qint2.data = Qint2\n",
    "    model.Kint2.data = Kint2\n",
    "    model.Vint2.data = Vint2\n",
    "\n",
    "    print(\"Loaded model with separate heads for domain1, domain2, interdomain\")\n",
    "\n",
    "    # (B) We load or define the MSA test data\n",
    "    # e.g. a 'paired' FASTA with domain1+domain2\n",
    "    # or you can have domainA.fasta, domainB.fasta if separate\n",
    "    # We'll assume it's domain1||domain2 in one file\n",
    "\n",
    "    L_A = model.N_alpha  # 64\n",
    "    model.to('cuda')\n",
    "    test_fasta = f\"{cwd}/CODE/DataAttentionDCA/data/lisa_data/HK-RR_174_test.fasta\"\n",
    "    MSA_A, MSA_B = paired_fasta_to_labeled_array(test_fasta, L_A=L_A)\n",
    "    # MSA_A => shape (N, L_A), MSA_B => shape(N, L_B)\n",
    "\n",
    "    # (C) get correct vs. incorrect energies\n",
    "    corr_scores, incorr_scores = test_interdomain_energy(\n",
    "        model,\n",
    "        MSA_A,\n",
    "        MSA_B,\n",
    "        correct_criterion=\"index\"\n",
    "    )\n",
    "\n",
    "    # (D) Plot hist\n",
    "    import seaborn as sns\n",
    "    mypalette=sns.color_palette(\"Set2\")\n",
    "    plt.figure()\n",
    "    bin_width=5\n",
    "    def_val=1e5  # if you used a default penalty for mismatched species, etc.\n",
    "\n",
    "    # Exclude any huge def_val if you do species checks\n",
    "    incorr_scores_filtered = incorr_scores[incorr_scores < def_val]\n",
    "\n",
    "    minval = min(corr_scores.min(), incorr_scores_filtered.min())\n",
    "    maxval = max(corr_scores.max(), incorr_scores_filtered.max())\n",
    "    bins = np.arange(minval, maxval+bin_width, bin_width)\n",
    "\n",
    "    plt.hist(corr_scores, bins=bins, alpha=0.5, label=\"Correct Pairs\", color=mypalette[0], density=True)\n",
    "    plt.hist(incorr_scores_filtered, bins=bins, alpha=0.5, label=\"Incorrect Pairs\", color=mypalette[1], density=True)\n",
    "\n",
    "    plt.xlabel(\"Inter-domain energy (domain1->domain2)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(f\"Comparison of correct vs. random pairs, H={H}, d={d}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

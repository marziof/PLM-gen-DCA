{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from model import AttentionModel\n",
    "from dcascore import *\n",
    "from utils import read_fasta_alignment, remove_duplicate_sequences\n",
    "# back to original path (in PLM)\n",
    "sys.path.pop(0)\n",
    "from plm_gen_methods import generate_plm_n_save\n",
    "from plm_seq_utils import nums_to_letters, modify_seq, letters_to_nums, set_seed, read_tensor_from_txt\n",
    "from plm_hamming_dist import vectorized_hamming_distance\n",
    "from plm_PCA import plot_pca_of_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bae41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "H = 64\n",
    "d= 10\n",
    "N = 174\n",
    "n_epochs = 500\n",
    "loss_type = 'without_J'\n",
    "family = 'jdoms' #'jdoms_bacteria_train2'\n",
    "#cwd = '/Users/marzioformica/Desktop/EPFL/Master/MA2/Labo/my_project/PLM-gen-DCA/Attention-DCA-main/CODE/AttentionDCA_python/src'\n",
    "cwd=r'C:\\Users\\youss\\OneDrive\\Bureau\\master epfl\\MA2\\TP4 De los Rios\\git_test\\PLM-gen-DCA\\Attention-DCA-main\\CODE\\AttentionDCA_python\\src'\n",
    "\n",
    "# Q_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_youss/Q_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "# K_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_youss/K_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "# V_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_youss/V_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "Q_1 = read_tensor_from_txt( cwd +r'\\results\\{H}_{d}_{family}_{losstype}_{n_epochs}_youss\\Q_tensor.txt'.format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "K_1 = read_tensor_from_txt( cwd +r'\\results\\{H}_{d}_{family}_{losstype}_{n_epochs}_youss\\K_tensor.txt'.format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "V_1 = read_tensor_from_txt( cwd +r'\\results\\{H}_{d}_{family}_{losstype}_{n_epochs}_youss\\V_tensor.txt'.format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "H,d,N=Q_1.shape\n",
    "q=V_1.shape[1]\n",
    "\n",
    "##############################################################\n",
    "\"\"\"\n",
    "    Initialize the model and compute couplings J from Q, K, V\n",
    "\"\"\" \n",
    "model=AttentionModel(H,d,N,q,Q=Q_1,V=V_1,K=K_1)\n",
    "torch.sum(model.Q-Q_1)\n",
    "device = Q_1.device\n",
    "L = Q_1.shape[-1]\n",
    "W=attention_heads_from_model(model,Q_1,K_1,V_1)\n",
    "print(W.shape)\n",
    "\n",
    "i_indices = torch.arange(L, device=device).unsqueeze(1)\n",
    "j_indices = torch.arange(L, device=device).unsqueeze(0)\n",
    "mask = (i_indices != j_indices).float().unsqueeze(0)  # shape (1, L, L)\n",
    "W = W * mask\n",
    "    \n",
    "# Compute Jtens\n",
    "Jtens = torch.einsum('hri,hab->abri', W, V_1)  # Shape: (q, q, L, L)\n",
    "q = Jtens.shape[0]\n",
    "N = Jtens.shape[2]\n",
    "print(q)\n",
    "print(N)\n",
    "\n",
    "\n",
    "beta_list=[5,2,0.5,0.1,0.01]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_test_data=r\"C:\\Users\\youss\\OneDrive\\Bureau\\master epfl\\MA2\\TP4 De los Rios\\git_test\\PLM-gen-DCA\\Attention-DCA-main\\CODE\\DataAttentionDCA\\jdoms\\jdoms_bacteria_train2.fasta\"\n",
    "seq_data_test=read_fasta_alignment(file_test_data,0.8)\n",
    "mean_ham_dist=[]\n",
    "print(seq_data_test.shape)\n",
    "seq_data_test_filtered,_=remove_duplicate_sequences(seq_data_test)\n",
    "seq_data_test_filtered=seq_data_test_filtered.T\n",
    "for beta in beta_list:\n",
    "    save_dir = \"generated_sequences\"\n",
    "    N_seqs = 10000\n",
    "    save_name = f\"plm_generated_sequences_{N_seqs}_beta_{beta}\"\n",
    "    init_sequence = 'DYYQVLGVPKDADAKSIKKAFRKLARKYHPDVNPGDKEAERKFKEANEANEVLSDPEKRKKYD'\n",
    "    init_sequence_num = letters_to_nums(init_sequence)\n",
    "    ratio = 0.2\n",
    "    init_sequence_num = modify_seq(init_sequence_num, ratio)\n",
    "    generate_plm_n_save(save_dir, save_name, Jtens, N_seqs, init_sequence=init_sequence_num)\n",
    "    filename = save_name\n",
    "    #filename = 'generated_sequences_randinit_40000'\n",
    "    #filename = 'generated_sequences_10000'\n",
    "    #cwd = '/Users/marzioformica/Desktop/EPFL/Master/MA2/Labo/my_project/PLM-gen-DCA/Attention-DCA-main'\n",
    "    cwd='C:\\Users\\youss\\OneDrive\\Bureau\\master epfl\\MA2\\TP4 De los Rios\\git_test\\PLM-gen-DCA\\Attention-DCA-main'\n",
    "    # Load the generated sequences\n",
    "    #output_file = cwd + f'/CODE/AttentionDCA_python/src/PLM/generated_sequences/{filename}.npy'\n",
    "\n",
    "    output_file = cwd + f'\\CODE\\AttentionDCA_python\\src\\PLM\\generated_sequences\\{filename}.npy'\n",
    "    gen_sequences = np.load(output_file)\n",
    "    mean_ham_dist.append(np.mean(vectorized_hamming_distance(gen_sequences,seq_data_test_filtered)))\n",
    "    fig_file=cwd + f'\\CODE\\AttentionDCA_python\\src\\PLM\\PCA_fig\\PCA_beta_{beta}'\n",
    "    plot_pca_of_sequences(gen_sequences,title=f\"beta={beta}\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1b44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

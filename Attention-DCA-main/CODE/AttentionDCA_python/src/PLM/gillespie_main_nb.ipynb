{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009f5e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 63, 63])\n",
      "21\n",
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [18:00<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequences (letters): ['YYYWVLGVPKDADPKSIKKAFRKLARKYHPDVNPGDKEAERKFKEANIANEVLLDPEKRFKYD', 'YYYWVLGVPKDSDPKSIKKAFRKLARKYHPDVNPGDKEAERKFKEANIANEVLLDPEKRFKYD', 'YYYWVLGVPKDSDPKSIKKAFRKLARKYHPDVNPGDKEAERKFKEANITNEVLLDPEKRFKYD', 'YYYWVLGVPKDSDPKSIKKAFKKLARKYHPDVNPGDKEAERKFKEANITNEVLLDPEKRFKYD', 'YYYWVLGVPKDSDPKSVKKAFKKLARKYHPDVNPGDKEAERKFKEANITNEVLLDPEKRFKYD']\n",
      "Generated sequences saved to gill_generated_sequences\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from model import AttentionModel\n",
    "from dcascore import *\n",
    "\n",
    "# back to original path (in PLM)\n",
    "sys.path.pop(0)\n",
    "from gillespie import SequenceGill\n",
    "from plm_seq_utils import nums_to_letters, modify_seq, letters_to_nums, set_seed, read_tensor_from_txt\n",
    "\n",
    "#-----------------------------------Functions--------------------------------------------\n",
    "\n",
    "def gennerate_gill(J,N_seqs=40000, init_sequence=None, beta=1):\n",
    "    gen_sequences = []\n",
    "    time_seq=[]\n",
    "    seq = SequenceGill(J, init_sequence, beta=1)\n",
    "    for _ in tqdm(range(N_seqs)):\n",
    "        seq.draw_aa()\n",
    "        gen_sequences.append(seq.sequence.copy())\n",
    "        time_seq.append(seq.time)\n",
    "    gen_sequences = np.array(gen_sequences)\n",
    "    time_seq = np.array(time_seq)\n",
    "    return gen_sequences, time_seq\n",
    "\n",
    "def generate_gill_n_save(save_dir, save_name, J, N_seqs=10000, init_sequence=None):\n",
    "    \"\"\"\n",
    "    Generates a set of sequences using Gillespie and saves them both as a numpy file and a text file containing the corresponding letter sequences.\n",
    "    Saves:\n",
    "    - A `.npy` file containing the generated sequences in numerical format.\n",
    "    - A `.txt` file containing the generated sequences in letter format.\n",
    "    \"\"\"\n",
    "    gen_sequences, time_seq = gennerate_gill(J, N_seqs, init_sequence)\n",
    "    gen_sequences_letters = [nums_to_letters(sequence) for sequence in gen_sequences]\n",
    "    \n",
    "    print(f\"Generated sequences (letters): {gen_sequences_letters[:5]}\")  # Show first 5 sequences\n",
    "    \n",
    "    gen_sequences = np.array(gen_sequences)\n",
    "        \n",
    "    # Check if the directory exists, create it if not\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Save the sequences in numerical format as a .npy file\n",
    "    np.save(f\"{save_dir}/{save_name}.npy\", gen_sequences)\n",
    "    np.save(f\"{save_dir}/{save_name}_time_seq.npy\", time_seq)\n",
    "    # Save the sequences in letter format as a .txt file (each sequence on a new line)\n",
    "    with open(f\"{save_dir}/{save_name}.txt\", \"w\") as f:\n",
    "        for sequence in gen_sequences_letters:\n",
    "            f.write(f\"{sequence}\\n\")\n",
    "\n",
    "    print(f\"Generated sequences saved to {save_dir}\")\n",
    "\n",
    "\n",
    "#-----------------------------------Main--------------------------------------------\n",
    "\n",
    "##############################################################\n",
    "\"\"\"\n",
    "    Load Q, K, V matrices from jdoms (after training)\n",
    "\"\"\"\n",
    "set_seed()\n",
    "H = 64\n",
    "d= 10\n",
    "N = 174\n",
    "n_epochs = 500\n",
    "loss_type = 'without_J'\n",
    "family = 'jdoms' #'jdoms_bacteria_train2'\n",
    "#cwd = '/Users/marzioformica/Desktop/EPFL/Master/MA2/Labo/my_project/PLM-gen-DCA/Attention-DCA-main/CODE/AttentionDCA_python/src'\n",
    "cwd=r'C:\\Users\\youss\\OneDrive\\Bureau\\master epfl\\MA2\\TP4 De los Rios\\git_test\\PLM-gen-DCA\\Attention-DCA-main\\CODE\\AttentionDCA_python\\src'\n",
    "\n",
    "# Q_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_youss/Q_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "# K_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_youss/K_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "# V_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_youss/V_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "Q_1 = read_tensor_from_txt( cwd +r'\\results\\{H}_{d}_{family}_{losstype}_{n_epochs}_youss\\Q_tensor.txt'.format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "K_1 = read_tensor_from_txt( cwd +r'\\results\\{H}_{d}_{family}_{losstype}_{n_epochs}_youss\\K_tensor.txt'.format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "V_1 = read_tensor_from_txt( cwd +r'\\results\\{H}_{d}_{family}_{losstype}_{n_epochs}_youss\\V_tensor.txt'.format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "H,d,N=Q_1.shape\n",
    "q=V_1.shape[1]\n",
    "\n",
    "##############################################################\n",
    "\"\"\"\n",
    "    Initialize the model and compute couplings J from Q, K, V\n",
    "\"\"\" \n",
    "model=AttentionModel(H,d,N,q,Q=Q_1,V=V_1,K=K_1)\n",
    "torch.sum(model.Q-Q_1)\n",
    "device = Q_1.device\n",
    "L = Q_1.shape[-1]\n",
    "W=attention_heads_from_model(model,Q_1,K_1,V_1)\n",
    "print(W.shape)\n",
    "\n",
    "i_indices = torch.arange(L, device=device).unsqueeze(1)\n",
    "j_indices = torch.arange(L, device=device).unsqueeze(0)\n",
    "mask = (i_indices != j_indices).float().unsqueeze(0)  # shape (1, L, L)\n",
    "W = W * mask\n",
    "    \n",
    "# Compute Jtens\n",
    "Jtens = torch.einsum('hri,hab->abri', W, V_1)  # Shape: (q, q, L, L)\n",
    "q = Jtens.shape[0]\n",
    "N = Jtens.shape[2]\n",
    "print(q)\n",
    "print(N)\n",
    "\n",
    "##############################################################\n",
    "\"\"\"\n",
    "    Generate sequences with PLM random initialization\n",
    "\"\"\"\n",
    "save_dir = \"gill_generated_sequences\"\n",
    "N_seqs = 10000\n",
    "save_name = \"gill_generated_sequences_randinit_10000\"\n",
    "#generate_gill_n_save(save_dir, save_name, Jtens, N_seqs=10000, init_sequence=None)\n",
    "\n",
    "##############################################################\n",
    "\"\"\"\n",
    "    Generate sequences with PLM initialization from a sequence\n",
    "\"\"\"\n",
    "init_sequence = 'DYYQVLGVPKDADAKSIKKAFRKLARKYHPDVNPGDKEAERKFKEANEANEVLSDPEKRKKYD'\n",
    "init_sequence_num = letters_to_nums(init_sequence)\n",
    "ratio = 0.1\n",
    "init_sequence_num = modify_seq(init_sequence_num, ratio)\n",
    "N_seqs=10000\n",
    "save_name = f\"gill_gen_seqs_w_init_seq_Ns{N_seqs}_r{ratio}\"\n",
    "generate_gill_n_save(save_dir, save_name, Jtens, N_seqs, init_sequence=init_sequence_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
